{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f61d31-74f1-44ba-9f97-9af24055478a",
   "metadata": {},
   "source": [
    "# 02 Data Preprocessing\n",
    "\n",
    "## Overview:\n",
    "This notebook handles data preprocessing for the **Animal Subspecies Classification** project. It includes:\n",
    "1. **Cleaning**: Removing corrupted, and non-image files.\n",
    "2. **Standardizing**: Resizing all images to a consistent size.\n",
    "3. **Splitting**: Dividing the dataset into training, validation, and testing subsets.\n",
    "\n",
    "## Goals:\n",
    "- Ensure the dataset is clean, standardized, and ready for model training.\n",
    "- Organize the data into folders for easy loading during model development.\n",
    "\n",
    "## Outputs:\n",
    "- Cleaned and standardized dataset, organized into training, validation, and testing subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dfbbf48-3c18-408f-af3f-d687ce194664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import shutil\n",
    "from hashlib import md5\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "\n",
    "# Data splitting\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de5d5c4-9cdb-4f9b-9153-4cc39339c916",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Purpose:\n",
    "This block cleans the raw dataset by:\n",
    "1. Removing corrupted or non-image files.\n",
    "\n",
    "## How It Works:\n",
    "- Each image is opened using the `PIL.Image` library to check if it is a valid image.\n",
    "- Logs are printed to indicate which files were removed and why.\n",
    "\n",
    "## Output:\n",
    "A clean dataset in the `data/raw` folder, ready for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "481bad5f-fbfe-4ca4-b693-407e775074d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning raw data...\n",
      "Data cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning\n",
    "def clean_data(source_dir):\n",
    "    \"\"\"\n",
    "    Removes corrupted or non-image files.\n",
    "    \"\"\"\n",
    "    for class_name in os.listdir(source_dir):\n",
    "        class_path = os.path.join(source_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            try:\n",
    "                # Verify image can be opened\n",
    "                with Image.open(img_path) as img:\n",
    "                    img.verify()  # Check if it's a valid image\n",
    "            except Exception as e:\n",
    "                print(f\"Removing corrupted or non-image file: {img_path} ({e})\")\n",
    "                os.remove(img_path)\n",
    "\n",
    "# Run data cleaning\n",
    "print(\"Cleaning raw data...\")\n",
    "clean_data(\"data/raw\")\n",
    "print(\"Data cleaning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ccdb1-3b2c-40d2-a832-4ac370b29333",
   "metadata": {},
   "source": [
    "# Image Standardization\n",
    "\n",
    "## Purpose:\n",
    "This block resizes all images to a consistent size (`224x224`) for compatibility with neural network models.\n",
    "\n",
    "## How It Works:\n",
    "- Each image in the `data/raw` folder is opened, resized to the target dimensions, and saved in the `data/processed` folder.\n",
    "- This step ensures that all input images are standardized, reducing the preprocessing overhead during model training.\n",
    "\n",
    "## Output:\n",
    "Standardized images saved in the `data/processed` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db5ba785-bab4-4d22-b676-c6931f97a1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing images...\n",
      "Image standardization complete!\n"
     ]
    }
   ],
   "source": [
    "# Function to standardize images (resize to 224x224)\n",
    "def standardize_images(source_dir, target_dir, image_size=(224, 224)):\n",
    "    for class_name in os.listdir(source_dir):\n",
    "        class_path = os.path.join(source_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        target_class_dir = os.path.join(target_dir, class_name)\n",
    "        os.makedirs(target_class_dir, exist_ok=True)\n",
    "\n",
    "        for img_name in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_name)\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    img = img.resize(image_size)\n",
    "                    img.save(os.path.join(target_class_dir, img_name))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "# Standardize images\n",
    "print(\"Standardizing images...\")\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "standardize_images(\"data/raw\", \"data/processed\")\n",
    "print(\"Image standardization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e6b8e-0449-4e48-9c52-4d8ab7865755",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "\n",
    "## Purpose:\n",
    "This block splits the standardized dataset into three subsets:\n",
    "1. **Training Set (70%)**: Used to train the neural network models.\n",
    "2. **Validation Set (20%)**: Used to tune hyperparameters and monitor overfitting.\n",
    "3. **Testing Set (10%)**: Used to evaluate the model's final performance.\n",
    "\n",
    "## How It Works:\n",
    "- Images are grouped by class.\n",
    "- Each class is split into training, validation, and testing subsets using `train_test_split` from `sklearn`.\n",
    "- The split is stratified to maintain class balance in each subset.\n",
    "- Images are moved into their respective folders (`data/train`, `data/val`, `data/test`).\n",
    "\n",
    "## Output:\n",
    "1. Training data in `data/train`.\n",
    "2. Validation data in `data/val`.\n",
    "3. Testing data in `data/test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b72b7408-c725-4f1f-9698-b0e8e3cf4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train, val, test...\n",
      "Data splitting complete!\n"
     ]
    }
   ],
   "source": [
    "# Function to split data into train, val, test\n",
    "def split_data(source_dir, train_dir, val_dir, test_dir, train_ratio=0.7, val_ratio=0.2):\n",
    "    for class_name in os.listdir(source_dir):\n",
    "        class_path = os.path.join(source_dir, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        images = os.listdir(class_path)\n",
    "        train, temp = train_test_split(images, train_size=train_ratio, random_state=42)\n",
    "        val, test = train_test_split(temp, test_size=(1 - train_ratio - val_ratio) / (val_ratio + (1 - train_ratio)))\n",
    "\n",
    "        # Move images to train, val, test folders\n",
    "        for img_name in train:\n",
    "            os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "            shutil.copy(os.path.join(class_path, img_name), os.path.join(train_dir, class_name, img_name))\n",
    "\n",
    "        for img_name in val:\n",
    "            os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
    "            shutil.copy(os.path.join(class_path, img_name), os.path.join(val_dir, class_name, img_name))\n",
    "\n",
    "        for img_name in test:\n",
    "            os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "            shutil.copy(os.path.join(class_path, img_name), os.path.join(test_dir, class_name, img_name))\n",
    "\n",
    "# Split data into train, val, test\n",
    "print(\"Splitting data into train, val, test...\")\n",
    "os.makedirs(\"data/train\", exist_ok=True)\n",
    "os.makedirs(\"data/val\", exist_ok=True)\n",
    "os.makedirs(\"data/test\", exist_ok=True)\n",
    "split_data(\"data/processed\", \"data/train\", \"data/val\", \"data/test\")\n",
    "print(\"Data splitting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896d26a-0566-4f66-9b2d-7949ec19843d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
